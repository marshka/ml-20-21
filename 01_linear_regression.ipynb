{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshka/ml-20-21/blob/main/01_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNkc6gv9x5t"
      },
      "source": [
        "# Machine Learning SP 2020/2021\n",
        "\n",
        "Prof. Cesare Alippi   \n",
        "Andrea Cini ([`andrea.cini@usi.ch`](mailto:andrea.cini@usi.ch))   \n",
        "Ivan Marisca ([`ivan.marisca@usi.ch`](mailto:ivan.marisca@usi.ch))   \n",
        "Nelson Brochado ([`nelson.brochado@usi.ch`](mailto:nelson.brochado@usi.ch))   \n",
        "\n",
        "---\n",
        "\n",
        "# Lab 01: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSzcDPkmJ6qz"
      },
      "source": [
        "# 01.A) Let's collect some data\n",
        "... or let someone do it for us :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXrZ4FvtNCDT"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "print(boston.DESCR)\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJX9OqvOcZu"
      },
      "source": [
        "Data set of $n=506$ observations $\\{(\\mathbf x_1, y_1), (\\mathbf x_2, y_2) ,\\dots,(\\mathbf x_n, y_n)\\}$, where $\\mathbf x_i\\in\\mathbb R^{d}$, with $d=13$ and $y_i\\in\\mathbb R$. All the observations are stack to form\n",
        "\n",
        "$$\n",
        "X = \\left[ \n",
        "\\begin{array}{c}\n",
        "\\mathbf x_1\\\\\n",
        "\\mathbf x_2\\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf x_n\n",
        "\\end{array}\n",
        "\\right] \n",
        "\\in \\mathbb{R}^{n\\times d},\n",
        "\\qquad \n",
        "Y = \\left[ \n",
        "\\begin{array}{c}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{array}\n",
        "\\right] \n",
        "\\in \\mathbb{R}^{n}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRFryiYXNCdf"
      },
      "source": [
        "# Let's consider only the RM index, for now.\n",
        "x = X[:, 5]  \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x, y, c='k', marker='.');\n",
        "plt.xlabel(\"RM\");\n",
        "plt.ylabel(\"thousand dollars\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWfVU_hXOlu8"
      },
      "source": [
        "# 01.B) System model\n",
        "\n",
        "We assume there is a function $g(x)$ that links RM index to the house price:\n",
        "$$\n",
        "y = g(x) + \\eta\n",
        "$$\n",
        "where $\\eta \\sim N(0, \\sigma^2_\\eta)$.\n",
        "\n",
        "Every line in the plane (except the vertical ones) can be written in the form\n",
        "$$f(x; \\boldsymbol \\theta) = \\theta_0 + \\theta_1 x$$ \n",
        "with $\\boldsymbol \\theta = (\\theta_0, \\theta_1)$ and $\\theta_0,\\theta_1 \\in\\mathbb R$.\n",
        "\n",
        "We also assume that $g(.)$ is linear, that is, there exists $\\boldsymbol \\theta^o$ so that $ g(x) = f(x; \\boldsymbol \\theta^o)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQjXiWDRkxf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def my_lin_fun(x, theta):\n",
        "    y = theta[0] + x * theta[1]\n",
        "    return y\n",
        "\n",
        "# some guesses\n",
        "xx = np.array([4., 9.])\n",
        "plt.plot(xx, my_lin_fun(xx, [.1, 3.]))\n",
        "plt.plot(xx, my_lin_fun(xx, [-1., 5.]))\n",
        "plt.scatter(x, y, c='k', marker='.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XM0AHt6Rk8T"
      },
      "source": [
        "# 01.C) Model approximation\n",
        "\n",
        "Given a new value of $x$ our best prediction for $y$ is\n",
        "$$\\hat y = E[y] = E[f(x; \\boldsymbol \\theta^o) + \\eta] = f(x; \\boldsymbol \\theta^o) + E[\\eta] = f(x; \\boldsymbol \\theta^o).$$\n",
        "\n",
        "Since ${\\boldsymbol \\theta^o}$ is unknown, we estimate it from the data, by minimising \n",
        "$$ \\hat{\\boldsymbol \\theta} = \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\theta} \\sum_{i=1}^n \\left\\lVert y_i - f(x;\\boldsymbol \\theta) \\right\\rVert^2_2 $$\n",
        "\n",
        "Finally, we predict new house prices with \n",
        "$$\\hat y = f\\left(x; \\hat{\\boldsymbol \\theta}\\right).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfdvHYsQiKSb"
      },
      "source": [
        "## 01.D) Parameter estimation\n",
        "\n",
        "__Data in compact form:__ Prepending a '1' to each $\\mathbf x$, then any \n",
        "$f(\\mathbf x,\\boldsymbol \\theta)= \\mathbf x^\\top \\boldsymbol \\theta$. In fact, $\\mathbf x^\\top \\boldsymbol \\theta = \\theta_0 1 +\\theta_1 x$.\n",
        "\n",
        "We showed in class that the solution $\\hat{\\boldsymbol \\theta}$ to $\\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\theta} \\sum_{i=1}^n \\left\\lVert y_i - f(x;\\boldsymbol \\theta) \\right\\rVert^2_2$ \n",
        "can be found by solving the linear system\n",
        "$$\n",
        "X^\\top Y = X^\\top X \\boldsymbol \\theta\n",
        "$$\n",
        "with respect to the $\\boldsymbol \\theta$.\n",
        "\n",
        "In our 1-D case,\n",
        "$$\n",
        "X = \\left[ \n",
        "\\begin{array}{c}\n",
        "1, x_1 \\\\\n",
        "1, x_2 \\\\\n",
        "\\vdots \\\\\n",
        "1, x_n\n",
        "\\end{array}\n",
        "\\right] \n",
        "\\in \\mathbb{R}^{n\\times 2},\n",
        "\\qquad \n",
        "Y = \\left[ \n",
        "\\begin{array}{c}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{array}\n",
        "\\right] \n",
        "\\in \\mathbb{R}^{n}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfBx7O5sZw6q"
      },
      "source": [
        "# Solving the linear system\n",
        "x_col_vec = x.reshape(-1, 1)\n",
        "ones_col_vec = np.ones(shape=(y.shape[0], 1))\n",
        "X = np.hstack((ones_col_vec, x_col_vec))\n",
        "theta_hat = np.linalg.solve(a=X.T.dot(X), b=X.T.dot(y))  # solves ax=b wrt x\n",
        "print(theta_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38pagQFna7bv"
      },
      "source": [
        "# plot the result\n",
        "plt.plot(xx, my_lin_fun(xx, theta_hat), 'g')\n",
        "plt.scatter(x, y, c='k', marker='.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX6kECiZbzfI"
      },
      "source": [
        "# There are also libraries for this\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# init the model\n",
        "lr = LinearRegression(fit_intercept=False)  \n",
        "\n",
        "# estimate parameters\n",
        "lr.fit(X, y)\n",
        "theta_hat2 = lr.coef_\n",
        "\n",
        "print('theta1 = {}'.format(theta_hat))\n",
        "print('theta2 = {}'.format(theta_hat2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FLmt1kEdCn9"
      },
      "source": [
        "# We can also avoid creating a column of ones\n",
        "lr = LinearRegression(fit_intercept=True)  # default is True  \n",
        "lr.fit(x_col_vec, y)\n",
        "theta_hat3 = [lr.intercept_, lr.coef_[0]]\n",
        "print('theta3 = {}'.format(theta_hat3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nZ-Fm0dm1x"
      },
      "source": [
        "# 01.E) More generally \n",
        "\n",
        "## i. Multidimensional data\n",
        "\n",
        "When the input (regressor) $\\mathbf x=[x_1,\\dots,x_d]$ is $d$-dimensional, then\n",
        "$$y = f(\\mathbf x, \\boldsymbol \\theta)  + \\eta = \\mathbf x^\\top \\boldsymbol \\theta = x_1 \\theta_1 + x_2 \\theta_2+ \\dots + x_d\\theta_d + \\eta.$$\n",
        "with $\\mathbf x,\\boldsymbol \\theta \\in\\mathbb R^d$.\n",
        "\n",
        "When also the output (target) $\\mathbf y$ is multidimensional:\n",
        "$$\\mathbf y = f(\\mathbf x, \\Theta) +\\eta= \\mathbf x^\\top \\Theta +\\eta$$\n",
        "with $\\mathbf y\\in\\mathbb R^f$, $\\Theta \\in\\mathbb R^{d\\times f}$ is a matrix and $\\eta\\sim N(0,I\\sigma_\\eta)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrnkOkEhg9CE"
      },
      "source": [
        "# We can also avoid creating a column of ones\n",
        "lr_1d = LinearRegression()  \n",
        "lr_1d.fit(x_col_vec, y)\n",
        "y_pred_1d = lr_1d.predict(x_col_vec)\n",
        "err_1d = ((y_pred_1d - y)**2).sum()\n",
        "\n",
        "# Let's try adding LSTAT...\n",
        "lr_2d = LinearRegression()  \n",
        "X2 = np.vstack((boston.data[:, 5], boston.data[:, 12])).T\n",
        "lr_2d.fit(X2, y)\n",
        "y_pred_2d = lr_2d.predict(X2)\n",
        "err_2d = ((y_pred_2d - y)**2).sum()\n",
        "\n",
        "# ... and finally with all the features \n",
        "lr_md = LinearRegression()  \n",
        "Xall = boston.data\n",
        "lr_md.fit(Xall, y)\n",
        "y_pred_md = lr_md.predict(Xall)\n",
        "err_md = ((y_pred_md - y)**2).sum()\n",
        "\n",
        "print(err_1d)\n",
        "print(err_2d)\n",
        "print(err_md)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh0h2IsEimFV"
      },
      "source": [
        "# plot the result\n",
        "%matplotlib inline\n",
        "\n",
        "# 1-d\n",
        "xx = np.array([3., 10.])\n",
        "plt.plot(x_col_vec, y_pred_1d, 'g')\n",
        "plt.scatter(x_col_vec, y, c='k', marker='.');\n",
        "\n",
        "# 2-d\n",
        "from mpl_toolkits.mplot3d import Axes3D  # this is necessary for 3-d plots \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1, projection='3d');\n",
        "ax.view_init(elev=10., azim=0)\n",
        "ax.set_xlabel('x_5')\n",
        "ax.set_ylabel('x_12')\n",
        "ax.set_zlabel('y')\n",
        "ax.plot_trisurf(X2[:, 0], X2[:, 1], y_pred_2d, alpha=0.3, label='est fun');\n",
        "ax.scatter(X2[:, 0], X2[:, 1], y, c='k', marker='.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXOYwKgMdnCZ"
      },
      "source": [
        "## ii. 'Linear' means linear in the parameters\n",
        "\n",
        "The regressor can be of the form \n",
        "$$\\boldsymbol \\phi(\\mathbf x) = [\\phi_1(\\mathbf x), \\phi_2(\\mathbf x), \\dots, \\phi_m(\\mathbf x)]$$ \n",
        "for any collection of functions $\\phi_1,\\dots,\\phi_m$. \n",
        "Function $f$ become\n",
        "$$f(\\mathbf x, \\boldsymbol \\theta) = \\boldsymbol \\theta^\\top\\boldsymbol \\phi(\\mathbf x) =  \\theta_1 \\phi_1(\\mathbf x) + \\theta_2 \\phi_d(\\mathbf x) + \\dots + \\theta_d \\phi_d(\\mathbf x).$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5W-BU6oUZT"
      },
      "source": [
        "\n",
        "### Example: Polynomials\n",
        "\n",
        "$$f(x;\\boldsymbol \\theta) = \\theta_0 + x \\theta_1 + x^2 \\theta_2 + \\dots + x^d \\theta_d$$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKYw9sXpmtGn"
      },
      "source": [
        "def pol_fun(x):\n",
        "    return -1 -x - .1 * x**2 + .5*x**3\n",
        "\n",
        "# generate data\n",
        "n = 100\n",
        "X = np.linspace(-1, 2, n).reshape(n,1) \n",
        "interval = np.linspace(-1, 2, 100).reshape(100,1)\n",
        "sigma = 0.3\n",
        "eta = np.random.normal(loc=0, scale=sigma, size=(n,1))\n",
        "Y = pol_fun(X) + eta\n",
        "\n",
        "# create regressor\n",
        "degree = 1\n",
        "\n",
        "# Xpol = np.concatenate((X, X**2, X**3), axis=1)\n",
        "from sklearn.preprocessing import PolynomialFeatures \n",
        "pol_feat = PolynomialFeatures(degree=degree, include_bias=False) \n",
        "Xpol = pol_feat.fit_transform(X)\n",
        "interval_pol = pol_feat.fit_transform(interval)\n",
        "\n",
        "# estimate parameter\n",
        "lr = LinearRegression()\n",
        "lr.fit(Xpol, Y)\n",
        "Y_est = lr.predict(interval_pol)\n",
        "\n",
        "# plot results\n",
        "plt.plot(interval, pol_fun(interval), label='true fun')\n",
        "plt.scatter(X, Y, label='noisy data', c='k', marker='.', alpha=0.5)\n",
        "plt.plot(interval, Y_est, label='est fun')\n",
        "plt.ylim((-2, 0.8))\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLuQeYuNn5hc"
      },
      "source": [
        "# 01.F) Regularizations\n",
        "\n",
        "## 1. Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQuCKsazC3KC"
      },
      "source": [
        "# generate data [200 100 50 20 10]\n",
        "n = 200\n",
        "X = np.linspace(-1, 2, n).reshape(n,1) \n",
        "sigma = 0.3\n",
        "eps = np.random.normal(loc=0, scale=sigma, size=(n,1))\n",
        "Y = pol_fun(X) + eps\n",
        "\n",
        "# create regressor\n",
        "degree = 5\n",
        "pol_feat = PolynomialFeatures(degree=degree, include_bias=False) \n",
        "Xpol = pol_feat.fit_transform(X)\n",
        "\n",
        "# linear regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(Xpol, Y)\n",
        "Y_est = lr.predict(Xpol)\n",
        "\n",
        "# ridge regression\n",
        "from sklearn.linear_model import Ridge \n",
        "rid = Ridge()\n",
        "rid.fit(Xpol, Y)\n",
        "Y_rid_est = rid.predict(Xpol)\n",
        "\n",
        "# plot results\n",
        "plt.plot(X, pol_fun(X), label='true fun')\n",
        "plt.scatter(X, Y, label='noisy data', c='k', marker='.')\n",
        "plt.plot(X, Y_est, label='lin.reg.')\n",
        "plt.plot(X, Y_rid_est, label='ridge reg.')\n",
        "plt.legend()\n",
        "\n",
        "# estimated theta\n",
        "print('theta_lr  = {}, {}'.format(lr.intercept_, lr.coef_[0]))\n",
        "print('theta_rid = {}, {}'.format(rid.intercept_, rid.coef_[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lQ_Uy_Anlyo"
      },
      "source": [
        "## ii. Lasso regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Y4YxP0y-bL"
      },
      "source": [
        "# Try yourself!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBhDMHDzJYy"
      },
      "source": [
        "# 01.G) More on linear regression\n",
        "## i. Confidence intervals for the parameters\n",
        "\n",
        "Assume that $X^\\top X$ is invertible, then\n",
        "\n",
        "$$\n",
        "\\hat \\theta = X^+Y \\sim N\\big(\\theta, \\sigma_\\eta^2 (X^\\top X)^{-1}\\big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "E[\\hat \\theta] = E[X^+Y] = X^+E[Y] = X^+ X\\theta = (X^\\top X)^{-1}X^\\top X \\theta = \\theta \n",
        "$$\n",
        "\n",
        "$$\n",
        "Var[\\hat \\theta] = Var[X^+Y] = X^+Var[Y] (X^+)^\\top = \\sigma_\\eta^2 (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} = \\sigma_\\eta^2 (X^\\top X)^{-1} \n",
        "$$\n",
        "\n",
        "A rule of thumb is the following\n",
        "\n",
        "* Extract the diagonal from $\\sigma_\\eta^2 (X^\\top X)^{-1}$, which gives you an idea of the variance of each component of $\\theta$.\n",
        "* For each component $\\theta_i$, check if the interval $(\\theta_i - 2\\sigma_i, \\theta_i + 2\\sigma_i)$ contains the zero; if that is the case, we are not very confident that the $\\theta_i\\ne 0$, thus that $x_i$ is relevant in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPnvcSgb-WdJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}