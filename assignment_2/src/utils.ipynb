{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCna+8lhPOTFr11nVnp+8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshka/ml-20-21/blob/main/assignment_2/src/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q9o1iFr2RR1"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVEhcaD2y2TK"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import urllib.request as http\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras import layers as keras_layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "\n",
        "\n",
        "def load_cifar10(num_classes=3):\n",
        "    \"\"\"\n",
        "    Downloads CIFAR-10 dataset, which already contains a training and test set,\n",
        "    and return the first `num_classes` classes.\n",
        "    Example of usage:\n",
        "\n",
        "    >>> (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    :param num_classes: int, default is 3 as required by the assignment.\n",
        "    :return: the filtered data.\n",
        "    \"\"\"\n",
        "    (x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
        "\n",
        "    fil_train = tf.where(y_train_all[:, 0] < num_classes)[:, 0]\n",
        "    fil_test = tf.where(y_test_all[:, 0] < num_classes)[:, 0]\n",
        "\n",
        "    y_train = y_train_all[fil_train]\n",
        "    y_test = y_test_all[fil_test]\n",
        "\n",
        "    x_train = x_train_all[fil_train]\n",
        "    x_test = x_test_all[fil_test]\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "def load_rps(download=False, path='rps', reduction_factor=1):\n",
        "    \"\"\"\n",
        "    Downloads the rps dataset and returns the training and test sets.\n",
        "    Example of usage:\n",
        "\n",
        "    >>> (x_train, y_train), (x_test, y_test) = load_rps()\n",
        "\n",
        "    :param download: bool, default is False but for the first call should be True.\n",
        "    :param path: str, subdirectory in which the images should be downloaded, default is 'rps'.\n",
        "    :param reduction_factor: int, factor of reduction of the dataset (len = old_len // reduction_factor).\n",
        "    :return: the images and labels split into training and validation sets.\n",
        "    \"\"\"\n",
        "    url = 'https://drive.switch.ch/index.php/s/xjXhuYDUzoZvL02/download'\n",
        "    classes = ('rock', 'paper', 'scissors')\n",
        "    rps_dir = os.path.abspath(path)\n",
        "    filename = os.path.join(rps_dir, 'data.zip')\n",
        "    if not os.path.exists(rps_dir) and not download:\n",
        "        raise ValueError(\"Dataset not in the path. You should call this function with `download=True` the first time.\")\n",
        "    if download:\n",
        "        os.makedirs(rps_dir, exist_ok=True)\n",
        "        print(f\"Downloading rps images in {rps_dir} (may take a couple of minutes)\")\n",
        "        path, msg = http.urlretrieve(url, filename)\n",
        "        with ZipFile(path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(rps_dir)\n",
        "        os.remove(filename)\n",
        "    train_dir, test_dir = os.path.join(rps_dir, 'train'), os.path.join(rps_dir, 'test')\n",
        "    print(\"Loading training set...\")\n",
        "    x_train, y_train = load_images_with_label(train_dir, classes)\n",
        "    x_train, y_train = x_train[::reduction_factor], y_train[::reduction_factor]\n",
        "    print(\"Loaded %d images for training\" % len(y_train))\n",
        "    print(\"Loading test set...\")\n",
        "    x_test, y_test = load_images_with_label(test_dir, classes)\n",
        "    x_test, y_test = x_test[::reduction_factor], y_test[::reduction_factor]\n",
        "    print(\"Loaded %d images for testing\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "def make_dataset(imgs, labels, label_map, img_size, rgb=True, keepdim=True, shuffle=True):\n",
        "    x = []\n",
        "    y = []\n",
        "    n_classes = len(list(label_map.keys()))\n",
        "    for im, l in zip(imgs, labels):\n",
        "        # preprocess img\n",
        "        x_i = im.resize(img_size)\n",
        "        if not rgb:\n",
        "            x_i = x_i.convert('L')\n",
        "        x_i = np.asarray(x_i)\n",
        "        if not keepdim:\n",
        "            x_i = x_i.reshape(-1)\n",
        "        \n",
        "        # encode label\n",
        "        y_i = np.zeros(n_classes)\n",
        "        y_i[label_map[l]] = 1.\n",
        "        \n",
        "        x.append(x_i)\n",
        "        y.append(y_i)\n",
        "    x, y = np.array(x).astype('float32'), np.array(y)\n",
        "    if shuffle:\n",
        "        idxs = np.arange(len(y))\n",
        "        np.random.shuffle(idxs)\n",
        "        x, y = x[idxs], y[idxs]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def load_images(path):\n",
        "    img_files = os.listdir(path)\n",
        "    imgs, labels = [], []\n",
        "    for i in img_files:\n",
        "        if i.endswith('.jpg'):\n",
        "            # load the image (here you might want to resize the img to save memory)\n",
        "            imgs.append(Image.open(os.path.join(path, i)).copy())\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def load_images_with_label(path, classes):\n",
        "    imgs, labels = [], []\n",
        "    for c in classes:\n",
        "        # iterate over all the files in the folder\n",
        "        c_imgs = load_images(os.path.join(path, c))\n",
        "        imgs.extend(c_imgs)\n",
        "        labels.extend([c] * len(c_imgs))\n",
        "    return imgs, labels\n",
        "\n",
        "\n",
        "def save_keras_model(model, filename):\n",
        "    \"\"\"\n",
        "    Saves a Keras model to disk.\n",
        "    Example of usage:\n",
        "\n",
        "    >>> model = Sequential()\n",
        "    >>> model.add(Dense(...))\n",
        "    >>> model.compile(...)\n",
        "    >>> model.fit(...)\n",
        "    >>> save_keras_model(model, 'my_model.h5')\n",
        "\n",
        "    :param model: the model to save;\n",
        "    :param filename: string, path to the file in which to store the model.\n",
        "    :return: the model.\n",
        "    \"\"\"\n",
        "    save_model(model, filename)\n",
        "\n",
        "\n",
        "def load_keras_model(filename):\n",
        "    \"\"\"\n",
        "    Loads a compiled Keras model saved with models.save_model.\n",
        "\n",
        "    :param filename: string, path to the file storing the model.\n",
        "    :return: the model.\n",
        "    \"\"\"\n",
        "    model = load_model(filename)\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_vgg16(model, filename='nn_task2.pkl', additional_args=()):\n",
        "    \"\"\"\n",
        "    Optimize task2 model by only saving the layers after vgg16. This function\n",
        "    assumes that you only added Flatten and Dense layers. If it is not the case,\n",
        "    you should include into `additional_args` other layers' attributes you\n",
        "    need.\n",
        "\n",
        "    :param filename: string, path to the file in which to store the model.\n",
        "    :param additional_args: tuple or list, additional layers' attributes to be \n",
        "    saved. Default are ['units', 'activation', 'use_bias']\n",
        "    :return: the path of the saved model.\n",
        "    \"\"\"\n",
        "    filename = filename if filename.endswith('.pkl') else (filename + '.pkl')\n",
        "    args = ['units', 'activation', 'use_bias', 'name', *additional_args]\n",
        "    layers = []\n",
        "    for l in model.layers[1:]:\n",
        "        layer = dict()\n",
        "        layer['class'] = l.__class__.__name__\n",
        "        layer['kwargs'] = {k: getattr(l, k) for k in dir(l) if k in args}\n",
        "        if l.weights:\n",
        "            layer['weights'] = l.get_weights()\n",
        "        layers.append(layer)\n",
        "\n",
        "    with open(filename, 'wb') as fp:\n",
        "        pickle.dump(layers, fp)\n",
        "    \n",
        "    return os.path.abspath(filename)\n",
        "\n",
        "\n",
        "def load_vgg16(filename='nn_task2.pkl', img_h=224, img_w=224):\n",
        "    \"\"\"\n",
        "    Loads the model saved with save_vgg16.\n",
        "\n",
        "    :param filename: string, path to the file storing the model.\n",
        "    :param img_h: int, the height of the input image.\n",
        "    :param img_w: int, the width of the input image.\n",
        "    :return: the model.\n",
        "    \"\"\"\n",
        "    K.clear_session()\n",
        "\n",
        "    vgg16 = applications.VGG16(weights='imagenet',  \n",
        "                              include_top=False, \n",
        "                              input_shape=(img_h, img_w, 3))\n",
        "    model = Sequential()\n",
        "    model.add(vgg16)\n",
        "\n",
        "    with open(filename, 'rb') as fp:\n",
        "        layers = pickle.load(fp)\n",
        "    for l in layers:\n",
        "        cls = getattr(keras_layers, l['class'])\n",
        "        layer = cls(**l['kwargs'])\n",
        "        model.add(layer)\n",
        "        if 'weights' in l:\n",
        "            model.layers[-1].set_weights(l['weights'])\n",
        "    \n",
        "    model.trainable = False\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxvuMnmmzAfa"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_rps(download=True, reduction_factor=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vEIz3PKZzQt"
      },
      "source": [
        "## Save/load `vgg16` model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pn4tvKqMixE"
      },
      "source": [
        "from tensorflow.keras import Sequential, applications\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# since VGG16 was trained on high-resolution images using a low resolution might not be a good idea\n",
        "img_h, img_w = 224, 224\n",
        "\n",
        "# Build the VGG16 network and download pre-trained weights and remove the last dense layers.\n",
        "vgg16 = applications.VGG16(weights='imagenet',  \n",
        "                           include_top=False, \n",
        "                           input_shape=(img_h, img_w, 3))\n",
        "# Freezes the network weights\n",
        "vgg16.trainable = False\n",
        "\n",
        "# Now you can use vgg16 as you would use any other layer.\n",
        "# Example:\n",
        "\n",
        "net = Sequential()\n",
        "net.add(vgg16)\n",
        "net.add(Flatten())\n",
        "net.add(Dense(1))  # <- JUST AN EXAMPLE TO MAKE A WORKING NETWORK, DON'T COPY\n",
        "net.summary()\n",
        "\n",
        "# Save model\n",
        "path = save_vgg16(net)\n",
        "\n",
        "# Load model\n",
        "print(\"\\nReload model\\n\")\n",
        "loaded_net = load_vgg16(path)\n",
        "loaded_net.summary()\n",
        "\n",
        "assert len(net.weights) == len(loaded_net.weights)\n",
        "weights_are_equal = [tf.equal(w1, w2).numpy().all()\n",
        "                     for w1, w2 in zip(net.weights, loaded_net.weights)]\n",
        "if all(weights_are_equal):\n",
        "    print(\"\\nThe loaded model has the same weights of the original one.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}