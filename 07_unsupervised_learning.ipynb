{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_unsupervised_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fJl2L7sNQ-zX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshka/ml-20-21/blob/main/07_unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6oJGO3smnq"
      },
      "source": [
        "# Machine Learning SP 2020/2021\n",
        "\n",
        "Prof. Cesare Alippi   \n",
        "Andrea Cini ([`andrea.cini@usi.ch`](mailto:andrea.cini@usi.ch))   \n",
        "Ivan Marisca ([`ivan.marisca@usi.ch`](mailto:ivan.marisca@usi.ch))   \n",
        "Nelson Brochado ([`nelson.brochado@usi.ch`](mailto:nelson.brochado@usi.ch))\n",
        "\n",
        "---\n",
        "# Lab 07: Unsupervised learning\n",
        "\n",
        "In this lab, we will see practical applications of unsupervised learning techniques. \n",
        "\n",
        "We will focus on two main tasks: \n",
        "\n",
        "1. Clustering;\n",
        "3. Dimensionality reduction.\n",
        "\n",
        "We will use two datasets that we are now very familiar with:\n",
        " - [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)\n",
        " - [MNIST](https://keras.io/api/datasets/mnist/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJl2L7sNQ-zX"
      },
      "source": [
        "\n",
        "## 7.1 Clustering the Iris dataset\n",
        "\n",
        "---\n",
        "\n",
        "In this task, we analyze the Iris dataset by considering **only the features**, without the targets (i.e., the classes). Let's start by loading the dataset and getting a sense of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi5yKDwV8Sqn"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# load the data\n",
        "iris = load_iris()\n",
        "\n",
        "# list the keys\n",
        "print(list(iris.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNwmlzDz27Wz"
      },
      "source": [
        "print(iris['DESCR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5RucxZp06yS"
      },
      "source": [
        "# read the keys\n",
        "print('feature_names:\\n', iris['feature_names'])\n",
        "print()\n",
        "print('target_names:\\n', iris['target_names'])\n",
        "print()\n",
        "print('data:\\n', iris['data'][:10])\n",
        "print()\n",
        "print('target:\\n', iris['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHj7hStgKPH9"
      },
      "source": [
        "# extract data\n",
        "X = iris.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2jzbohUKT2F"
      },
      "source": [
        "**Remark:** This should be an **unsupervised** learning setup. So, even though `iris.target` is present, we assume to have **no label** associated with the data.\n",
        "\n",
        "Now let's see the shapes of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOcsH8-csmMt"
      },
      "source": [
        "print('Shape of X:', X.shape)\n",
        "\n",
        "(n, d) = X.shape\n",
        "print('d:', d)\n",
        "print('n:', n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95GxDfJb7yUj"
      },
      "source": [
        "### 7.1.1 Data visualization\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHAavOb7-UlW"
      },
      "source": [
        "#### Histograms\n",
        "\n",
        "Let's see the estimated pdf of each component (i.e., feature) by means of the histograms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXuT8k471qN"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "for i in range(d):\n",
        "    # a subplot for each feature\n",
        "    plt.subplot(1, d, i+1)\n",
        "\n",
        "    # histogram\n",
        "    plt.hist(X[:, i], density=True, color=f'C{i}')\n",
        "\n",
        "    # axis labels\n",
        "    plt.xlabel('$x_{}$: {}'.format(i, iris.feature_names[i]))\n",
        "    if i == 0:  plt.ylabel('estimated pdf')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkIzyTUHM9Ls"
      },
      "source": [
        "A couple of observations:\n",
        "\n",
        "* the different ranges\n",
        "* $x_2$ and $x_3$ are roughly bimodal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYBLsHDH6sFQ"
      },
      "source": [
        "\n",
        "#### Scatter plots \n",
        "\n",
        "We try to plot more features at the same time. We have 4 features but we can visualize at most 3D.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHZvlAeD1cAQ"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(15, 6))\n",
        "\n",
        "# x0, x1, x2\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_zlabel('$x_2$')\n",
        "\n",
        "# x0, x2, x3\n",
        "ax = fig.add_subplot(122, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 2], X[:, 3])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$x_3$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfIOdPHy68sv"
      },
      "source": [
        "It seems that we can identify in a easier way clusters by watching $x_0, x_2$ and $x_3$ jointly instead of $x_0, x_1$ and $x_2$. Can't we?\n",
        "Or maybe it just depends on where we are looking at the data from..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lD2yrpkPKYb"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 6))\n",
        "\n",
        "# x0, x1, x2\n",
        "ax = fig.add_subplot(121, projection='3d', elev=-150, azim=110)\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_zlabel('$x_2$')\n",
        "\n",
        "# x0, x2, x3\n",
        "ax = fig.add_subplot(122, projection='3d', elev=-150, azim=110)\n",
        "ax.scatter(X[:, 0], X[:, 2], X[:, 3])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$x_3$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPjfnHqLPRDo"
      },
      "source": [
        "Without the right perspective we may miss important clues.\n",
        "\n",
        "2D plots are usually clearer than 3D ones (personal opinion!), let's try with them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlNJSjX8cJv"
      },
      "source": [
        "def plot_every_pair(X, colors=None, same_axis=False, label_pfx=\"x\"):\n",
        "    d = X.shape[1]\n",
        "    if colors is None:\n",
        "        colors = np.zeros(X.shape[0])\n",
        "    n_plots = d*(d-1)//2\n",
        "    plt.figure(figsize=(3 * max(2, n_plots), 8))\n",
        "    ct = 0 \n",
        "    for i in range(1, d+1):\n",
        "        for j in range(i+1, d+1):\n",
        "            ct += 1\n",
        "            plt.subplot(2, max(1, n_plots//2), ct)\n",
        "            plt.scatter(X[:, i-1], X[:, j-1], c=colors)\n",
        "            plt.xlabel('${}_{}$'.format(label_pfx, i-1))\n",
        "            plt.ylabel('${}_{}$'.format(label_pfx, j-1))\n",
        "            if same_axis:\n",
        "                # Use same axis scaling\n",
        "                plt.xlim([X.min(), X.max()])\n",
        "                plt.ylim([X.min(), X.max()])\n",
        "    plt.show()\n",
        "\n",
        "plot_every_pair(X)               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2YvOsyVQII6"
      },
      "source": [
        "Be careful about the different ranges!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnwPRvkQNEb"
      },
      "source": [
        "plot_every_pair(X, same_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JpGgWB3QBF4"
      },
      "source": [
        "#### Seaborn\n",
        "\n",
        "A cool package for data visualization is `seaborn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcIRy8fPlak"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozdm9rms-vsQ"
      },
      "source": [
        "The above visualization is rather difficult when the number of feature is large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I70KdBun9jmp"
      },
      "source": [
        "### 7.1.2 Principal Component Analysis\n",
        "\n",
        "Recall the steps\n",
        "\n",
        "1. Let $X \\in \\mathbb{R}^{n \\times d}$ be the dataset\n",
        "1. Subtract the mean. Should we rescale?\n",
        "  $$ X \\leftarrow X - \\overline{X} $$\n",
        "1. Consider the sample covariance matrix\n",
        "  $$\\hat{\\Sigma} = \\frac{1}{n-1} X^\\top X$$\n",
        "1. Compute the symmetrical and semidefinite positive\n",
        "  $$H = X^\\top X$$\n",
        "  and its eigen-decomposition\n",
        "  $$ H = U \\Lambda U^\\top $$\n",
        "  where $U \\in \\mathbb{R}^{d \\times d}$ is the eigenvectors matrix and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is the eigenvalues matrix (diagonal).\n",
        "\n",
        "  **Remark 1:** Eigenvalues and eigenvectors: $H \\mathbf u = \\lambda \\mathbf u$\n",
        "\n",
        "1. Now apply the transformation\n",
        "  1. Lossless: apply $U^\\top \\mathbf x$ to each vector (simple rotation).\n",
        "  2. Lossy:\n",
        "    - Discard $l$ eigenvectors obtaining $\\tilde{U} \\in \\mathbb{R}^{d \\times d-l}$.\n",
        "    - apply transformation $\\tilde U^\\top \\mathbf x$ to each vector.\n",
        "\n",
        "  To transform the entire dataset, simply do $XU$ or $X\\tilde U$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDQf58ENAxuv"
      },
      "source": [
        "X_mean = np.mean(X, axis=0, keepdims=True)\n",
        "X0 = X - X_mean\n",
        "\n",
        "H = (X0.T).dot(X0)\n",
        "lam, U = np.linalg.eigh(H)\n",
        "\n",
        "print(\"shapes:\", lam.shape, U.shape)\n",
        "print(\"eigenvalues:\", lam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoTyknqWFMVN"
      },
      "source": [
        "We need to reverse `lam` and `U` since we want the eigenvalues to be sorted in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB4Ohwt9-YRN"
      },
      "source": [
        "# Sort the eigenvalues\n",
        "lam = lam[::-1]\n",
        "U = U[:, ::-1]\n",
        "\n",
        "plt.plot(lam, 'o-')\n",
        "plt.title(\"eigenvalues\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"component\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVozxQ8FBcD1"
      },
      "source": [
        "# Apply rotation\n",
        "X_rot = X0.dot(U)\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "for i in range(d):\n",
        "    plt.subplot(1, d, i+1)\n",
        "    plt.hist(X_rot[:, i])\n",
        "    plt.xlabel('$pc_{}$'.format(i))\n",
        "\n",
        "plot_every_pair(X_rot, same_axis=True, label_pfx=\"pc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRLx6rsl7BlE"
      },
      "source": [
        "# Apply reduced transformation\n",
        "l = 2  # columns to discard\n",
        "Utilde = U[:, :d-l]\n",
        "X_red = X0.dot(Utilde)\n",
        "# Equivalent to X_red = X_rot[:, :d-l]\n",
        "\n",
        "plot_every_pair(X_red, same_axis=True, label_pfx=\"pc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPF8hWfsyxiH"
      },
      "source": [
        "As usual, `sklearn` can speed up our work!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSBmDO-a8pSt",
        "scrolled": true
      },
      "source": [
        "# PCA with sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# d:    num of original features (= num of all principal components)\n",
        "# l:    num of discarded principal components\n",
        "# d-l:  num of considered principal components\n",
        "pca = PCA(n_components=d-l)\n",
        "pca.fit(X0)\n",
        "X_red = pca.transform(X0)\n",
        "# X_red[:, 0] = -X_red[:, 0]  # reverse dimension zero since corresponding eigenvector is symmetric wrt our solution \n",
        "\n",
        "plot_every_pair(X_red, same_axis=True, label_pfx=\"pc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYSb1UB6RZ-"
      },
      "source": [
        "### 7.1.3 Data reconstruction\n",
        "\n",
        "What about if we want to come back to the original number of components?\n",
        "\n",
        "$$\\mathbf x \\to \\mathbf {\\tilde x} \\to \\mathbf x_{rec} \\approx \\mathbf x$$\n",
        "\n",
        "where $\\mathbf x \\in \\mathbb{R}^{d}, \\mathbf {\\tilde x} \\in \\mathbb{R}^{d-l}$ and $\\mathbf x_{rec} \\in \\mathbb{R}^{d}$. In this way we are able to **compress** the original data in a low dimensional space and restore them (in a **lossy** way!).\n",
        "\n",
        "- Transformation: $\\mathbf{\\tilde x}=\\tilde U^\\top \\mathbf x$.\n",
        "- Reconstruction (inverse transformation): $\\mathbf x_{rec} = \\tilde U \\mathbf{\\tilde x}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnA1N7VUyBT_"
      },
      "source": [
        "# Visualize original vs reconstructed dataset\n",
        "fig = plt.figure(figsize=(18, 4))\n",
        "fig.subplots_adjust(wspace=.4)\n",
        "\n",
        "# Original dataset\n",
        "ax = fig.add_subplot(131, projection='3d', elev=30, azim=160)\n",
        "\n",
        "ax.scatter(X0[:, 0], X0[:, 2], X0[:, 3]) #, X[:, 3])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$x_3$')\n",
        "ax.set_title(\"$X$\")\n",
        "\n",
        "# Principal components\n",
        "ax = fig.add_subplot(132)\n",
        "ax.scatter(X_red[:, 0], X_red[:, 1])\n",
        "ax.set_xlabel('$pc_0$')\n",
        "ax.set_ylabel('$pc_1$')\n",
        "ax.set_title(\"Principal Components\")\n",
        "ax.axis(\"equal\")\n",
        "\n",
        "# Reconstructed dataset\n",
        "ax = fig.add_subplot(133, projection='3d', elev=30, azim=160)\n",
        "# reconstruct\n",
        "X_rec = pca.inverse_transform(X_red)\n",
        "# which is equivalent to \n",
        "# X_red_ = X.dot(Utilde)\n",
        "# X_rec_ = X_red_.dot(Utilde.T)\n",
        "\n",
        "ax.scatter(X_rec[:, 0], X_rec[:, 2], X_rec[:, 3]) #, X_rec[:, 3])\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$x_3$')\n",
        "ax.set_title(\"$X$ reconstructed\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbwfV_zUJQK7"
      },
      "source": [
        "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names))\n",
        "\n",
        "sns.pairplot(pd.DataFrame(X_red, columns=[f'$pc_{pc}$' for pc in range(d-l)]))\n",
        "\n",
        "sns.pairplot(pd.DataFrame(X_rec, columns=iris.feature_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmvk-zM9Tx6y"
      },
      "source": [
        "### 7.1.4 Clustering: k-means\n",
        "\n",
        "Now that we managed to represent the original dataset in a low dimensional space we can use the $k$-means clustering technique to classify the data into groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZSFEHNK6sjc"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_clusters = 2\n",
        "\n",
        "k_means = KMeans(n_clusters=k_clusters)\n",
        "cluster_label = k_means.fit_predict(X_red)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEBoYXMX7bHG"
      },
      "source": [
        "# 3d\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(121, projection='3d', elev=30, azim=160)\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_label)\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_zlabel('$x_2$')\n",
        "# 2d PC\n",
        "ax = fig.add_subplot(122)\n",
        "ax.scatter(X_red[:, 0], X_red[:, 1], c=cluster_label)\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.axis(\"equal\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPbfT-xz7n4A"
      },
      "source": [
        "Since we know that there are three classes in `iris.target`... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyRzLgYTDEj7"
      },
      "source": [
        "# 3d\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(121, projection='3d', elev=30, azim=160)\n",
        "for c, label in enumerate(iris.target_names):\n",
        "  idxs = iris.target == c\n",
        "  ax.scatter(X[idxs, 0], X[idxs, 1], X[idxs, 2], label=label)\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_zlabel('$x_2$')\n",
        "# 2d PC\n",
        "ax = fig.add_subplot(122)\n",
        "for c, label in enumerate(iris.target_names):\n",
        "  idxs = iris.target == c\n",
        "  ax.scatter(X_red[idxs, 0], X_red[idxs, 1], label=label)\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.axis(\"equal\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Classes (not clusters!)\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px4OtYmw6UsO"
      },
      "source": [
        "However, k-means (as well as any other clustering method) does not necessarily retrieve the same classes, because classes are not necessarily confined into clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUuGPCB_6TqT"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_clusters = 3\n",
        "\n",
        "k_means = KMeans(n_clusters=k_clusters)\n",
        "cluster_label = k_means.fit_predict(X_red)\n",
        "\n",
        "# 3d\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "ax = fig.add_subplot(131, projection='3d', elev=30, azim=160)\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_label)\n",
        "ax.set_xlabel(r'$x_0$')\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.set_zlabel(r'$x_2$')\n",
        "# 2d PC\n",
        "ax = fig.add_subplot(132)\n",
        "ax.scatter(X_red[:, 0], X_red[:, 1], c=cluster_label)\n",
        "ax.set_xlabel(r'$x_0$')\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.axis(\"equal\")\n",
        "ax.set_title(\"clusters\")\n",
        "\n",
        "# classes\n",
        "ax = fig.add_subplot(133)\n",
        "ax.scatter(X_red[:, 0], X_red[:, 1], c=iris.target)\n",
        "ax.set_xlabel(r'$x_0$')\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.axis(\"equal\")\n",
        "ax.set_title(\"classes\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGlmm-BfwRPI"
      },
      "source": [
        "#### Final considerations\n",
        "\n",
        "- We can cross-validate the number of clusters ([silhouette](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html))\n",
        "- Variety of clustering methods with different behaviours ([comparison](https://scikit-learn.org/stable/modules/clustering.html#clustering)) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTWiyb1b9-Mt"
      },
      "source": [
        "## 7.2 Image compression\n",
        "\n",
        "---\n",
        "\n",
        "In this part, we will see how we can compress an image dataset, reducing the number of components needed to represent each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is1xPVeW-ELd"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "dataset = 'mnist'  # 'cifar10'\n",
        "\n",
        "def plot_sample(imgs, labels, nrows, ncols, resize=None, tograyscale=False, shuffle=True):\n",
        "    # create a grid of images\n",
        "    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
        "    # take a random sample of images\n",
        "    if shuffle:\n",
        "        indices = np.random.choice(len(imgs), size=nrows*ncols, replace=False)\n",
        "    else:\n",
        "        indices = np.arange(nrows*ncols)\n",
        "    for ax, idx in zip(axs.reshape(-1), indices):\n",
        "        ax.axis('off')\n",
        "        # sample an image\n",
        "        ax.set_title(labels[idx])\n",
        "        im = imgs[idx]\n",
        "        if isinstance(im, np.ndarray):\n",
        "            im = Image.fromarray(im)  \n",
        "        if resize is not None:\n",
        "            im = im.resize(resize)\n",
        "        if tograyscale:\n",
        "            im = im.convert('L')\n",
        "        ax.imshow(im, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "# Load the data\n",
        "if dataset == 'mnist':\n",
        "  from tensorflow.keras.datasets import mnist\n",
        "  (x_train, y_train), _ = mnist.load_data()\n",
        "elif dataset == 'cifar10':\n",
        "  from tensorflow.keras.datasets import mnist, cifar10\n",
        "  (x_train, y_train), _ = cifar10.load_data()\n",
        "  (x_train, y_train) = (x_train.mean(-1), y_train.mean(-1))  # grayscale\n",
        "\n",
        "plot_sample(x_train, y_train, 2, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfH5vrGYNZWo"
      },
      "source": [
        "Vectorize the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0gdkrdK-PX0"
      },
      "source": [
        "n_samples = 60000\n",
        "x_train, y_train = x_train[:n_samples], y_train[:n_samples]\n",
        "print(\"x_train:\", x_train.shape) \n",
        "\n",
        "# Reshape to vectors and rescale to [0, 1]\n",
        "w, h = x_train.shape[1:3]\n",
        "X = x_train.reshape(-1, w * h) /255.\n",
        "print(\"X:\", X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNbUIuyFdF5h"
      },
      "source": [
        "Let's plot the principal components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJHcd8K9-a2Z"
      },
      "source": [
        "# PCA\n",
        "n_components = 300\n",
        "\n",
        "X_mean = X.mean(axis=0, keepdims=True)\n",
        "X0 = X - X_mean \n",
        "pca = PCA(n_components=n_components)\n",
        "pca.fit(X0)\n",
        "\n",
        "plt.figure()\n",
        "plt.title('eigenvalues')\n",
        "plt.plot(pca.singular_values_**2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVAEH-x8Hyy7"
      },
      "source": [
        "# compress\n",
        "X_red = pca.transform(X0)\n",
        "# extract\n",
        "X_rec = pca.inverse_transform(X_red)\n",
        "X_rec += X_mean\n",
        "\n",
        "# reshape to image size and range\n",
        "x_image_rec = 255 * X_rec.clip(0, 1).reshape(-1, w, h)\n",
        "x_image_orig = 255 * X.clip(0, 1).reshape(-1, w, h)\n",
        "\n",
        "# draw some random images\n",
        "p = np.random.choice(X.shape[0], size=5)\n",
        "print(\"Original images\")\n",
        "plot_sample(x_image_orig[p], y_train[p], 1, 5, shuffle=False)\n",
        "print(\"Reconstructed images\")\n",
        "plot_sample(x_image_rec[p], y_train[p], 1, 5, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47By6mUp47MO"
      },
      "source": [
        "### Image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr-KjJWldjEj"
      },
      "source": [
        "It seems that we are able to decently rebuild the original images using way less features! Now what happens if we add noise to the original images?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoioU09-_0lz"
      },
      "source": [
        "# Add noise to X\n",
        "X_noisy = X + np.random.randn(*X.shape)*.2\n",
        "\n",
        "# PCA\n",
        "n_components = 50\n",
        "X_mean = X_noisy.mean(axis=0, keepdims=True)\n",
        "X0 = X_noisy - X_mean \n",
        "pca = PCA(n_components=n_components)\n",
        "pca.fit(X0)\n",
        "\n",
        "plt.figure()\n",
        "plt.title('eigenvalues')\n",
        "plt.plot(pca.singular_values_**2)\n",
        "plt.show()\n",
        "\n",
        "# compress\n",
        "X_red = pca.transform(X0)\n",
        "# extract\n",
        "X_rec = pca.inverse_transform(X_red)\n",
        "X_rec += X_mean\n",
        "\n",
        "# reshape to image size and range\n",
        "x_image_orig = 255 * X.clip(0, 1).reshape(-1, w, h)\n",
        "x_image_noisy = 255 * X_noisy.clip(0, 1).reshape(-1, w, h)\n",
        "x_image_rec = 255 * X_rec.clip(0, 1).reshape(-1, w, h)\n",
        "\n",
        "# draw some random images\n",
        "p = np.random.choice(X.shape[0], size=5)\n",
        "print(\"Original images\")\n",
        "plot_sample(x_image_orig[p], y_train[p], 1, 5, shuffle=False)\n",
        "print(\"Noisy images\")\n",
        "plot_sample(x_image_noisy[p], y_train[p], 1, 5, shuffle=False)\n",
        "print(\"Reconstructed images\")\n",
        "plot_sample(x_image_rec[p], y_train[p], 1, 5, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOfev2beS23"
      },
      "source": [
        "Now let's see if using only two dimensions we are able to plot the dataset in a clusterized fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgKekvc7QYD_"
      },
      "source": [
        "x_dim, y_dim = 0, 1\n",
        "plt.figure(figsize=(16, 12))\n",
        "for d in range(10):\n",
        "    ii = np.where(y_train==d)[0]\n",
        "    plt.scatter(X_red[ii][:, x_dim], X_red[ii][:, y_dim], marker=f\"${d}$\", label=d)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}