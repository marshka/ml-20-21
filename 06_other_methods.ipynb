{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_other_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshka/ml-20-21/blob/main/06_other_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4HJeN686y2"
      },
      "source": [
        "# Machine Learning\n",
        "\n",
        "Prof. Cesare Alippi\\\n",
        "Andrea Cini ([`andrea.cini@usi.ch`](mailto:andrea.cini@usi.ch))\\\n",
        "Ivan Marisca ([`ivan.marisca@usi.ch`](mailto:ivan.marisca@usi.ch))\\\n",
        "Nelson Brochado ([`nelson.brochado@usi.ch`](mailto:nelson.brochado@usi.ch))\n",
        "\n",
        "---\n",
        "# Lab 06: Other methods\n",
        "\n",
        "In this lab we will see how to use some of the more advanced methods that we saw in the last lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fle838Qjy4I9"
      },
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "Let's see how we can use Support Vector Machines and the kernel trick to solve classification problems where linear approaches fail. \n",
        "\n",
        "Remeber, a **kernel** (oversimplifying a lot) is a function that gives us a particular measure of affinity between two points. We can use kernels in the dual formulation of the SVM problem to project the input space in a high (possibly infinite) dimensional space.\n",
        "\n",
        "Let's start defining our usual helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IgAvdbC1482"
      },
      "source": [
        "# first we define some helper functions to generate data and plot results\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "# function to generate classification problems\n",
        "def get_data(n, ctype='simple'):\n",
        "  if ctype == 'simple':\n",
        "    x, y = make_classification(n_features=2, \n",
        "                               n_redundant=0, \n",
        "                               n_informative=2, \n",
        "                               n_clusters_per_class=1)\n",
        "    x += np.random.uniform(size=x.shape) # add some noise\n",
        "  elif ctype == 'circles':\n",
        "    x, y = make_circles(n, noise=0.1, factor=0.5)\n",
        "  \n",
        "  elif ctype == 'moons':\n",
        "    x, y = make_moons(n, noise=0.1)\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return x, y\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x):    \n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .01  # step size in the mesh\n",
        "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "\n",
        "  # plot train data\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$');\n",
        "\n",
        "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, y_pred, alpha=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRAMU22KO6C"
      },
      "source": [
        "### The XOR problem\n",
        "\n",
        "As already discussed, linear methods are unable to solve problems in which the classes are not linearly separable. \n",
        "\n",
        "The XOR problem is the classic example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlSbxu8_jM5"
      },
      "source": [
        "np.random.seed(270)\n",
        "\n",
        "x = np.random.randn(200, 2) # sample some points from a bivariate diagonal gaussian\n",
        "y = np.logical_xor(x[:, 0] > 0., x[:, 1] > 0.)\n",
        "\n",
        "x[x > 0.] += .5\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhzsyXIi94NU"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def k(x1, x2):\n",
        "  return np.dot(x1, x2.T)\n",
        "\n",
        "# def k(x1, x2):\n",
        "#   gamma = .25\n",
        "#   return np.exp( - gamma * euclidean_distances(x1, x2) ** 2. )\n",
        "\n",
        "svm = SVC(kernel=k)\n",
        "\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7PyRPPTXzVW"
      },
      "source": [
        "As you can see using a linear kernel we are still limited to drawing hyperplanes, let's go back and try out something more interesting...\n",
        "\n",
        "\n",
        "Of course we do not need to define kernels functions by hand, they are already implemented in `scikit-learn`. We can also check which are the support vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiAuV7z0YvuM"
      },
      "source": [
        "svm = SVC(kernel='rbf', gamma=.25)\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)\n",
        "plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], marker='x', color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAEiFeQmuPEY"
      },
      "source": [
        "You can find more on the mathematical formulation of scikit-learn's SVC [here](https://scikit-learn.org/stable/modules/svm.html#svc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e98odkQYXQY"
      },
      "source": [
        "### Iris dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6JtuoMlqCq"
      },
      "source": [
        "As a second case study, we are going to use the [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset, where the objective is to classify flowers based on some features:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn7xqfdkmfCE"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        " \n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLg7L-imfjV"
      },
      "source": [
        "Let's give a look at the data. (we only use two features to make visualization easier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SjvNW28BxGR"
      },
      "source": [
        "x_prime = x[:, :2]\n",
        "\n",
        "plt.scatter(x_prime[:, 0], x_prime[:, 1], c=y)\n",
        "plt.xlabel('sepal length (cm)')\n",
        "plt.ylabel('sepal width (cm)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi0MniB74Q7a"
      },
      "source": [
        "classifier = SVC(kernel='rbf', gamma=1)\n",
        "\n",
        "classifier.fit(x_prime, y)\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwVoxFoEi1F"
      },
      "source": [
        "Tuning correctly the hyperparameters is fundamental (check [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)).\n",
        "\n",
        "In particular (for the `rbf` kernel):\n",
        "\n",
        "* `C` : cost of a misclassification error\n",
        "* `gamma`: $1 / \\sigma$ of the Gaussian kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Tbo-OmeBCR"
      },
      "source": [
        "# Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIvjOQ49Udr"
      },
      "source": [
        "Again we can use `scikitlearn` to build Decision Trees preatty easily in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayq8nv8xojjG"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier() # create an instance of the model\n",
        "classifier.fit(x_prime, y)            # fit the data\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMC63iSnWPJ"
      },
      "source": [
        "Watch out for overfitting!\n",
        "\n",
        "Now let's try to build a tree using all the features. To visualize the result we'll look directly at the tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7hQaTYdqx5T"
      },
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "classifier = DecisionTreeClassifier(criterion='entropy')\n",
        "classifier.fit(x, y)  \n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plot_tree(classifier, filled=True, feature_names=iris.feature_names, rounded=True, class_names=iris.target_names);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8oD-KbOnpGj"
      },
      "source": [
        "Decision Trees are easy to interpret and that's why they are really popular in financial applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj7Ee9MMegWI"
      },
      "source": [
        "# Random Forests\n",
        "\n",
        "We saw during classes that the expected test error can be written as\n",
        "\n",
        "$${error} = {bias}^2 + variance + noise$$\n",
        "\n",
        "In general we cannot reduce the noise, we can reduce the bias increasing model complexity, but this makes the variance increase too.\n",
        "\n",
        "Can we reduce the variance without increasing the bias and without getting more data? **Yes**\n",
        "\n",
        "\n",
        "\n",
        "The idea is to take the average prediction of $K$ models. In fact, if we consider our model to be a random variable $X$, the variance of the mean is lower than the variance of the population, i.e., ${Var}(\\overline X_K) \\le Var(X)$ (using the CLT: ${Var}(\\overline X_K) \\approx \\frac{Var(X)}{K})$.\n",
        "\n",
        "\n",
        "The problem is that we have only a single training set: we can use **bootstrapping** (i.e., sampling with replacement) to learn the K models from K sets of bootstrapped samples.\n",
        "\n",
        "This technique is known as **bagging** and works particularly well with trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKtdaJB5txTz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=20) \n",
        "model.fit(x_prime, y)           \n",
        "\n",
        "plot_decision_surface(model, x_prime, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB3hXM-mu02d"
      },
      "source": [
        "Being simple yet powerful, these methods are [very popular](https://storage.googleapis.com/kaggle-media/surveys/Kaggle%20State%20of%20Machine%20Learning%20and%20Data%20Science%202020.pdf) for practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7FVNyyuRoQ"
      },
      "source": [
        "## Pointers to some popular (different) methods\n",
        "\n",
        "* `xgboost`: https://xgboost.readthedocs.io/en/latest/\n",
        "* `lighgbm`: https://lightgbm.readthedocs.io/en/latest/\n",
        "\n",
        "Note: these methods are based on **boosting**, an approach based on the idea of using a set of weak learners to iteratively reduce the error."
      ]
    }
  ]
}