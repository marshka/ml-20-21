{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshka/ml-20-21/blob/main/02_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4HJeN686y2"
      },
      "source": [
        "# Machine Learning\n",
        "\n",
        "Prof. Cesare Alippi\\\n",
        "Andrea Cini ([`andrea.cini@usi.ch`](mailto:andrea.cini@usi.ch))\\\n",
        "Ivan Marisca ([`ivan.marisca@usi.ch`](mailto:ivan.marisca@usi.ch))\\\n",
        "Nelson Brochado ([`nelson.brochado@usi.ch`](mailto:nelson.brochado@usi.ch))\n",
        "\n",
        "---\n",
        "# Lab 02: Classification\n",
        "\n",
        "We have a $d$-dimensional input vector $X \\in \\mathbb{R}^d$ and a set of $k$ possible classes, $C_1, \\dots, C_k$.\n",
        "Our goal in classification is to assign $X$ to the correct class. \n",
        "\n",
        "In particular, our goal is to determine a __discriminant__ function to parition the input space. In this session we will focus on binary classification.\n",
        "\n",
        "![alt text](http://atriplex.info/blog/wp-content/uploads/2017/05/th_lda.png)\n",
        "\n",
        "---\n",
        "# Logistic Regression\n",
        "\n",
        "Consider a binary classification problem $\\{(x_1, y_1), \\dots, (x_1, y_n)\\}$ and a Logistic Regression model, then:\n",
        "\n",
        "$$Pr(y_i=1\\vert x_i, \\boldsymbol \\theta) = \\sigma(x_i^\\top\\boldsymbol \\theta) = \\frac{1}{1+e^{-x_i^\\top\\boldsymbol \\theta}}\n",
        "$$\n",
        "\n",
        "Where $\\sigma({}\\cdot{})$ is the _sigmoid_ function:\n",
        "\n",
        "<img style=\"text-align: center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/2560px-Logistic-curve.svg.png\" width=\"400\">\n",
        "\n",
        "Let's get some data and use _scikit-learn_ to fit a Logistic Regression model on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IgAvdbC1482"
      },
      "source": [
        "# first we define some helper functions to generate data and plot results\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# color_maps\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])  \n",
        "\n",
        "# function to generate classification problems\n",
        "def get_data(n, ctype='simple'):\n",
        "  if ctype == 'simple':\n",
        "    x, y = make_classification(n_features=2, \n",
        "                               n_redundant=0, \n",
        "                               n_informative=2, \n",
        "                               n_clusters_per_class=1)\n",
        "    x += np.random.uniform(size=x.shape) # add some noise\n",
        "  elif ctype == 'circles':\n",
        "    x, y = make_circles(n, noise=0.1, factor=0.5)\n",
        "  \n",
        "  elif ctype == 'moons':\n",
        "    x, y = make_moons(n, noise=0.1)\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return x, y\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x):    \n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .01  # step size in the mesh\n",
        "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "\n",
        "  # plot train data\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright,\n",
        "              edgecolors='k')\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$');\n",
        "\n",
        "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, y_pred > 0.5, cmap=cm, alpha=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SjvNW28BxGR"
      },
      "source": [
        "np.random.seed(78)\n",
        "\n",
        "# Create a classification problem\n",
        "x, y = get_data(100)\n",
        "\n",
        "# Let's look at the data\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayq8nv8xojjG"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression() # create an instance of the model\n",
        "classifier.fit(x, y)              # fit the data\n",
        "\n",
        "plot_decision_surface(classifier, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4giXAY4TPV"
      },
      "source": [
        "Logistic Regression is often referred to as a **generalized linear method**. In fact, even if the model is nonlinear, the predicted class depends only on the linear combination $x_i^\\top \\boldsymbol \\theta$ of the input variables. In other words, the decision surfaces are **linear**.\n",
        "\n",
        "In fact, the decision boundary of the binary logistic regression model that we built can be written as:\n",
        "\n",
        "$$x_2 = -\\frac{\\theta_0}{\\theta_2} - \\frac{\\theta_1}{\\theta_2} x_1$$\n",
        "\n",
        "_Homework: show why this is true._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi0MniB74Q7a"
      },
      "source": [
        "theta = np.c_[classifier.intercept_, classifier.coef_].ravel()\n",
        "b = -theta[0]/theta[2]\n",
        "m = -theta[1]/theta[2]\n",
        "\n",
        "x1 = np.array([x[:,0].min(), x[:,0].max()])\n",
        "x2 = b + m * x1\n",
        "\n",
        "plt.plot(x1, x2, c='black')\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwVoxFoEi1F"
      },
      "source": [
        "What happens if the data are not linerly separable?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlSbxu8_jM5"
      },
      "source": [
        "np.random.seed(20)\n",
        "\n",
        "x,y = get_data(120, 'circles')\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(x, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcvcieFmGv5T"
      },
      "source": [
        "Remember: the linearity is in the input variables! We can use some nonlinear features to project the data in a space where they are separable with a straight line.\n",
        "\n",
        "Let's try with polar coordinates.\n",
        "$$\n",
        "\\left\\{\\begin{array}{rl}\n",
        "x&=r \\cos \\phi \\\\\n",
        "y&=r \\sin \\phi\n",
        "\\end{array}\n",
        "\\right. \\implies\n",
        "\\left\\{\n",
        "  \\begin{array}{rl}\n",
        "r&=\\sqrt{x^2 + y^2}\\\\\n",
        "\\phi&= {atan2}\\left(y,x\\right) \n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "<img src=\"https://vvvv.org/sites/default/files/imagecache/large/images/283px-Polar_coordinate_components.svg_.png\" width=\"300\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs15ChyY_kTA"
      },
      "source": [
        "def to_polar(x):\n",
        "  r = np.sqrt(np.square(x).sum(axis=1))\n",
        "  phi = np.arctan2(x[:,1], x[:,0])\n",
        "  return np.c_[r, phi]\n",
        "\n",
        "x_polar = to_polar(x)\n",
        "\n",
        "plt.scatter(x_polar[:, 0], x_polar[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
        "plt.xlabel(r'$r$')\n",
        "plt.ylabel(r'$\\phi$');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6nKbEKIyRn"
      },
      "source": [
        "Nice! Let's fit the model using the polar features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A5AKM1GAMah"
      },
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(x_polar, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y, transform=to_polar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzlJtIhDJ4cO"
      },
      "source": [
        "It is not always easy to find suitable features/projections by hand. That's why we need nonlinearity.\n",
        "\n",
        "In the next part we will see how to use neural networks to solve classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gttJ8kwLVdh"
      },
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igyFScSyLehY"
      },
      "source": [
        "# try it on your own!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ5s722RHacM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}